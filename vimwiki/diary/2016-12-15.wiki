= December 15th =

= Contents =
    - [[#December 15th|December 15th]]
    - [[#Data science interview questions|Data science interview questions]]
    - [[#Kaggle - Things that didnt work|Kaggle - Things that didnt work]]
    - [[#gtypist|gtypist]]
    - [[#Vimwiki Custom HTML Templates|Vimwiki Custom HTML Templates]]
    - [[#Factorization Machines|Factorization Machines]]
        - [[#Factorization Machines#Relation to SVM|Relation to SVM]]
            - [[#Factorization Machines#Relation to SVM#Linear SVM|Linear SVM]]
            - [[#Factorization Machines#Relation to SVM#Polynomial SVM d = 2|Polynomial SVM d = 2]]

= Data science interview questions =
    - [[https://www.springboard.com/blog/data-science-interview-questions/|link]]

= Kaggle - Things that didnt work =
    - Owens LOO Encoding 
        - [[https://www.kaggle.com/c/allstate-claims-severity/forums/t/25101/owen-s-loo-encoding|link]]
    - IDs!!
    - Nvidia GTX 1080 brings per-epoch time down from 25-80 secs to 1-5 secs
        - costs $600!!
    - Using lower "eta" values does not always lead to best results
    - Converting this regression problem to a classification problem
        - Bin Loss into classes by:
            - Equal Interval Sizes/Width
            - Equal Frequency/Percentiles
            - Mutual Information

= gtypist =
    - Complete Lesson 1
    
= Vimwiki Custom HTML Templates =
[[http://www.rosipov.com/blog/custom-templates-in-vimwiki/|link]]


= Factorization Machines =
    - Like SVM, model interactions between features.
    - Paper: [[http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf]] (very simple to follow)
    - Model Equation for d=2 degree interactions:
    {{$%align%
    \hat{y}  (\mathbf x) = w_0 + \sum_i w_i x_i + \sum_{(i,j): i \neq j} \left<\mathbf v_i, \mathbf v_j\right> x_i x_j
    }}$
    where model parameters are:
        - $ w_0 \in \mathbb R $
        - $ w_i \in \mathbb R    \,\forall\,  i \in \left[n\right] $
        - $ \mathbf v_i \in \mathbb R^k  \,\forall\,  i \in \left[n\right] $
        - $ k $ is a hyper-parameter that describes the dimensionality of the factorization.
    - Supposedly FM works better than SVM for sparse problems 
    - Model equation for a FM can be computed in linear time $ O(kn) $.
    - Model parameters can be computed efficiently using gradient descent.
    
    == Relation to SVM ==
    - Model equation for SVM can be expressed as dot-product between transformed input $ \mathbf x $ and model parameters $ \mathbf w $:
    {{$
    \hat{y} (\mathbf x) = \left< \phi(\mathbf x), \mathbf w \right>
    }}$
    where $\phi$ is mapping from feature space to more complex space. The mapping is related to the kernel with:
    {{$
    K: \mathbb R^n \times \mathbb R^n \rightarrow \mathbb R, \quad K(\mathbf x, \mathbf z) = \left< \phi(\mathbf x), \phi(\mathbf z ) \right>
    }}$
        
        === Linear SVM ===
        - Kernel $K(\mathbf x,\mathbf z) = 1 + \left< \mathbf x, \mathbf z \right>$
        - Mapping $\phi(\mathbf x) = (1, x_1, x_2, \ldots, x_n)$
        - Identical to FM with $d = 1$
        
        === Polynomial SVM d = 2 ===
        - Kernel $ K(\mathbf x, \mathbf z) = (1 + \left< \mathbf x, \mathbf z \right>)^d$
        - Mapping for d = 2, $ \phi(\mathbf x) = (1, \sqrt 2 x_1, \ldots, \sqrt 2 x_n, x_1^2, \ldots, x_n^2, \sqrt 2 x_1 x_2, \ldots, \sqrt 2 x_{n-1} x_n ) $
        - Model equation: $ \hat{y} (\mathbf x) = \left< \phi(\mathbf x), \mathbf w \right> = w_0 + \sqrt 2 \sum_i w_i x_i + \sqrt 2 \sum_{(i,j): i \neq j} w_{(i,j)} x_i x_j + \sum_i w_{(i,i)} x_i^2 $
        - Model parameters $w_0, \mathbf w, W = [w_{(i, j)}]$
        - The main difference between SVMs and FMs is the parametrization: all interaction parameters $w_{(i,j)}$ of SVMs are completely independent, e.g. $w_{(i,j)}$ and $w_{(i,l)}$.
        - In contrast to this the interaction parameters of FMs are factorized and thus $\left< \mathbf v_i, \mathbf v_j \right>$ and $\left< \mathbf v_i, \mathbf v_l \right>$ depend on each other as they share parameter $\mathbf v_i$

    - The dense parametrization of SVMs requires direct observations for the interactions which is often not given in sparse settings.
    - Collaborative Filtering example:
        - $x_1$ denotes User Alice and $x_{10}$ denotes Movie Titanic,
        - Then estimating model parameter $w_{(1,10)}$ needs observations of rating of Titanic by Alice,
        - (x_1 or x_10 are 0 for other observations),
        - But if it is available in training data, it not in test data.
        - _Thus the polynomial SVM can make no use of any 2-way interaction for predicting test examples._

    - The model equation of FMs is independent of the training data. Prediction with SVMs depends on parts of the training data (the support vectors).
